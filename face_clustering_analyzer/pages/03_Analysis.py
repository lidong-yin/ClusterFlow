from __future__ import annotations

import math
from typing import Any, Optional

import numpy as np
import pandas as pd
import streamlit as st

st.set_page_config(page_title="Analysis - Face Clustering Analyzer", layout="wide", initial_sidebar_state="expanded")

from src import analysis_utils, data_utils, faiss_utils, ui_utils
from src.state import KEYS, ensure_state, get_df, get_df_rev, get_faiss, get_feature_col, set_faiss, set_feature_col

ui_utils.load_app_style()


def _require_df():
    df = get_df()
    if df is None:
        st.info("è¯·å…ˆåœ¨ **Home** é¡µé¢åŠ è½½æ•°æ®ã€‚")
        return None
    return df


def _detect_group_keys(df: pd.DataFrame) -> list[str]:
    keys: list[str] = []
    if "gt_person_id" in df.columns:
        keys.append("gt_person_id")
    keys += data_utils.detect_cluster_label_columns(df)
    # fallback: allow any column
    if not keys:
        keys = list(df.columns)
    return keys


def _get_features(df: pd.DataFrame, *, feature_col: str, ok_only: bool, progress_callback=None):
    ok_col = "ok" if "ok" in df.columns else None
    feats, row_idx = data_utils.extract_feature_matrix(
        df,
        feature_col=feature_col,
        ok_col=ok_col,
        ok_only=ok_only,
        progress_callback=progress_callback,
    )
    return feats, row_idx


def _analysis_cache() -> dict:
    cache = st.session_state.get(KEYS.analysis_cache)
    if not isinstance(cache, dict):
        cache = {}
        st.session_state[KEYS.analysis_cache] = cache
    return cache


def _cache_key(prefix: str, *parts: Any) -> str:
    safe = [str(p) for p in parts]
    return prefix + "::" + "::".join(safe)


def _get_group_index_map(df: pd.DataFrame, group_key: str) -> dict[Any, Any]:
    cache = _analysis_cache()
    k = _cache_key("group_index_map", get_df_rev(), group_key)
    if k in cache:
        return cache[k]
    # dropna=False to keep NaN as a group
    m = df.groupby(group_key, dropna=False).groups
    cache[k] = m
    return m


def _set_state(key: str, value: Any) -> None:
    st.session_state[key] = value


def _slice_indices(indices: Any, start: int, end: int):
    # works for list-like / pandas Index
    return indices[start:end]


def _sample_by_indices(
    df: pd.DataFrame,
    indices: Any,
    *,
    n: int,
    random_sample: bool,
    seed: int,
) -> pd.DataFrame:
    total = int(len(indices))
    n = int(max(1, n))
    if total == 0:
        return df.iloc[0:0]
    if total <= n:
        return df.loc[indices]
    if random_sample:
        rng = np.random.default_rng(int(seed))
        # Convert to numpy array for fast choice; avoid copying large arrays if possible
        idx_arr = np.asarray(indices)
        chosen = rng.choice(idx_arr, size=n, replace=False)
        return df.loc[chosen]
    return df.loc[_slice_indices(indices, 0, n)]


def _render_cluster_header(
    *,
    title: str,
    subtitle: str,
    chips: list[str],
) -> None:
    chip_html = "".join([f'<span class="chip">{c}</span>' for c in chips])
    st.markdown(
        f"""
<div class="cluster-header">
  <div>
    <div class="cluster-title">{title}</div>
    <div class="cluster-sub">{subtitle}</div>
  </div>
  <div class="chips">{chip_html}</div>
</div>
""",
        unsafe_allow_html=True,
    )


def _toggle_key(*parts: Any) -> str:
    return _cache_key("ui_toggle", *parts)


def _expanded_key(*parts: Any) -> str:
    return _cache_key("ui_expanded", *parts)


def _page_key(*parts: Any) -> str:
    return _cache_key("ui_page", *parts)


def _get_expand_state(
    *,
    total: int,
    collapsed_n: int,
    cfg: dict,
    key_parts: list[Any],
) -> tuple[bool, int, int]:
    """
    Compute (expanded, page, per_page) from session_state without rendering widgets.
    """
    total = int(total)
    collapsed_n = int(collapsed_n)
    per_page = max(1, int(cfg["img_cols"]) * int(cfg["page_rows"]))
    
    if total <= collapsed_n:
        return False, 1, per_page

    expanded_state_key = _expanded_key(get_df_rev(), *key_parts)
    expanded = bool(st.session_state.get(expanded_state_key, False))
    
    if not expanded:
        return False, 1, per_page

    num_pages = int(np.ceil(total / per_page))
    page_key = _page_key(get_df_rev(), *key_parts)
    page = int(st.session_state.get(page_key, 1))
    page = max(1, min(page, num_pages))
    return True, page, per_page


def _render_expand_trigger(
    *,
    total: int,
    collapsed_n: int,
    cfg: dict,
    key_parts: list[Any],
) -> None:
    """Render the button/pagination controls AFTER the grid."""
    total = int(total)
    collapsed_n = int(collapsed_n)
    if total <= collapsed_n:
        return

    expanded_state_key = _expanded_key(get_df_rev(), *key_parts)
    expanded = bool(st.session_state.get(expanded_state_key, False))
    label = "æ”¶èµ·" if expanded else f"æŸ¥çœ‹å…¨éƒ¨ ({total:,})"
    
    # Button row
    # Use columns to keep button small/aligned left or center
    c1, _ = st.columns([1, 5])
    with c1:
        if st.button(label, key=_toggle_key(get_df_rev(), *key_parts), use_container_width=True):
            st.session_state[expanded_state_key] = not expanded
            st.session_state[_page_key(get_df_rev(), *key_parts)] = 1
            st.rerun()
            
    if expanded:
        per_page = max(1, int(cfg["img_cols"]) * int(cfg["page_rows"]))
        num_pages = int(np.ceil(total / per_page))
        if num_pages > 1:
            cur = int(st.session_state.get(_page_key(get_df_rev(), *key_parts), 1))
            st.slider(
                "é¡µç ",
                min_value=1,
                max_value=num_pages,
                value=cur,
                step=1,
                key=_page_key(get_df_rev(), *key_parts),
            )



def _stable_seed(x: Any) -> int:
    # Deterministic small int seed for sampling
    return abs(hash(str(x))) % 1000003


def _expected_faiss_meta(cfg: dict) -> dict[str, Any]:
    return {
        "df_rev": get_df_rev(),
        "feature_col": cfg["feature_col"],
        "ok_only": bool(cfg["ok_only"]),
        "use_gpu_prefer": bool(cfg["use_gpu_prefer"]),
    }


def _ensure_faiss_index(df: pd.DataFrame, cfg: dict, *, progress=None):
    index, _, row_idx = get_faiss()
    meta = st.session_state.get(KEYS.faiss_meta)
    expected = _expected_faiss_meta(cfg)
    if index is not None and row_idx is not None and meta == expected:
        return index, row_idx

    prog = progress or st.progress(0.0, text="æ„å»º Faiss ç´¢å¼•ï¼šæå–ç‰¹å¾ ...")

    def feat_cb(frac: float, text: str) -> None:
        prog.progress(min(0.35, 0.35 * float(frac)), text=text)

    feats, row_idx = _get_features(df, feature_col=cfg["feature_col"], ok_only=cfg["ok_only"], progress_callback=feat_cb)

    # Normalize in-place to support cosine via IP
    prog.progress(0.38, text="æ„å»º Faiss ç´¢å¼•ï¼šL2 normalize ...")
    feats = feats.astype(np.float32, copy=False)
    norms = np.linalg.norm(feats, axis=1, keepdims=True)
    norms = np.maximum(norms, 1e-12)
    feats /= norms

    prog.progress(0.42, text="æ„å»º Faiss ç´¢å¼•ï¼šåˆå§‹åŒ– index ...")
    index = faiss_utils.build_index_ip(feats, use_gpu_prefer=cfg["use_gpu_prefer"])

    n = int(feats.shape[0])
    add_bs = 50000
    for start in range(0, n, add_bs):
        end = min(start + add_bs, n)
        index.add(feats[start:end])
        prog.progress(0.42 + 0.55 * (end / max(1, n)), text=f"æ„å»º Faiss ç´¢å¼•ï¼šadd {end:,}/{n:,}")

    set_faiss(index, None, row_idx)
    st.session_state[KEYS.faiss_meta] = expected
    prog.progress(1.0, text="Faiss ç´¢å¼•å°±ç»ª")
    return index, row_idx


def _sidebar_controls(df: pd.DataFrame) -> dict[str, Any]:
    st.sidebar.header("åˆ†æè®¾ç½®")

    group_keys = _detect_group_keys(df)
    group_key = st.sidebar.selectbox("åˆ†ç»„ä¾æ® (Grouping Key)", options=group_keys, index=0)

    view_mode = st.sidebar.radio("åˆ†æè§†è§’ (View Mode)", options=["Size", "Variance", "Scatter"], index=0)
    desc = st.sidebar.checkbox("é™åºæ’åº (desc)", value=True)

    st.sidebar.divider()
    st.sidebar.subheader("èŒƒå›´ / å±•ç¤º")
    default_start = int(st.session_state.get(KEYS.group_range_start, 1))
    default_end = int(st.session_state.get(KEYS.group_range_end, 50))
    start = st.sidebar.number_input("èµ·å§‹ç»„åºå· (1-based)", min_value=1, value=default_start, step=1)
    end = st.sidebar.number_input("ç»“æŸç»„åºå· (inclusive)", min_value=1, value=max(default_end, start), step=1)
    st.session_state[KEYS.group_range_start] = int(start)
    st.session_state[KEYS.group_range_end] = int(end)

    img_cols = st.sidebar.slider("å›¾ç‰‡åˆ—æ•°", min_value=6, max_value=20, value=int(st.session_state.get(KEYS.img_cols, 12)), step=1)
    st.session_state[KEYS.img_cols] = int(img_cols)

    per_group_images = st.sidebar.slider("æ¯ç°‡å±•ç¤ºå›¾ç‰‡æ•°", min_value=10, max_value=200, value=20, step=5)
    random_sample = st.sidebar.checkbox("æ¯ç°‡éšæœºé‡‡æ ·å±•ç¤ºï¼ˆå›ºå®šç§å­ï¼‰", value=False)
    page_rows = st.sidebar.slider("å±•å¼€æ—¶æ¯é¡µè¡Œæ•°", min_value=5, max_value=30, value=10, step=1)

    st.sidebar.subheader("å…ƒæ•°æ®æ˜¾ç¤º")
    meta_cols = st.sidebar.multiselect(
        "é€‰æ‹©å›¾ç‰‡ä¸‹æ–¹æ˜¾ç¤ºçš„å­—æ®µ",
        options=ui_utils.columns_except(df, exclude=["feature"]),
        default=[],
    )

    st.sidebar.divider()
    st.sidebar.subheader("æŸ¥è¯¢æœç´¢")
    search_mode = st.sidebar.selectbox("æœç´¢æ¨¡å¼", options=["cluster_id / label", "obj_id"], index=0)
    search_value = st.sidebar.text_input("æœç´¢å€¼", value="")
    show_topk = st.sidebar.checkbox("obj_id æœç´¢åæ˜¾ç¤º TopK ç›¸ä¼¼æ ·æœ¬ï¼ˆéœ€è¦ Faiss+featureï¼‰", value=True)
    sim_topk = st.sidebar.number_input("TopK", min_value=1, max_value=2048, value=20, step=1)

    st.sidebar.divider()
    st.sidebar.subheader("1v1 ç›¸ä¼¼åº¦")
    obj_a = st.sidebar.text_input("obj_id A", value="")
    obj_b = st.sidebar.text_input("obj_id B", value="")

    st.sidebar.divider()
    st.sidebar.subheader("Feature / Faiss")
    feature_candidates = [c for c in df.columns if c.lower() in {"feature", "feat", "embedding", "emb"}]
    if "feature" in df.columns and "feature" not in feature_candidates:
        feature_candidates.insert(0, "feature")
    if not feature_candidates:
        feature_candidates = list(df.columns)
    feature_col = st.sidebar.selectbox(
        "ç‰¹å¾åˆ— (feature)",
        options=feature_candidates,
        index=feature_candidates.index(get_feature_col()) if get_feature_col() in feature_candidates else 0,
    )
    if feature_col != get_feature_col():
        set_feature_col(feature_col)

    ok_only = st.sidebar.checkbox("åªä½¿ç”¨ ok==True çš„æ ·æœ¬ï¼ˆè‹¥å­˜åœ¨ ok åˆ—ï¼‰", value=True)
    use_gpu_prefer = st.sidebar.checkbox("ä¼˜å…ˆä½¿ç”¨ GPU Faissï¼ˆè‹¥å¯ç”¨ï¼‰", value=True)

    # Scatter params
    sim_th = st.sidebar.number_input("Scatter sim_th", min_value=-1.0, max_value=1.0, value=0.55, step=0.01)
    scatter_topk = st.sidebar.number_input("Scatter sim_topk", min_value=1, max_value=2048, value=100, step=1)
    cand_limit = st.sidebar.number_input("æ¯ç»„æ˜¾ç¤ºå€™é€‰ç°‡æ•°", min_value=1, max_value=20, value=3, step=1)
    dedup_scatter = st.sidebar.checkbox("æ•£åº¦ç»“æœå»é‡ (A-B vs B-A)", value=False)

    return {
        "group_key": group_key,
        "view_mode": view_mode,
        "desc": bool(desc),
        "range_start": int(start),
        "range_end": int(end),
        "img_cols": int(img_cols),
        "per_group_images": int(per_group_images),
        "random_sample": bool(random_sample),
        "page_rows": int(page_rows),
        "meta_cols": meta_cols,
        "search_mode": search_mode,
        "search_value": search_value.strip(),
        "show_topk": bool(show_topk),
        "sim_topk": int(sim_topk),
        "obj_a": obj_a.strip(),
        "obj_b": obj_b.strip(),
        "feature_col": feature_col,
        "ok_only": bool(ok_only),
        "use_gpu_prefer": bool(use_gpu_prefer),
        "sim_th": float(sim_th),
        "scatter_topk": int(scatter_topk),
        "cand_limit": int(cand_limit),
        "dedup_scatter": bool(dedup_scatter),
    }


def _locate_by_label(groups_df: pd.DataFrame, group_key: str, value: str) -> Optional[int]:
    if value == "":
        return None
    # Try int first
    try:
        v = int(value)
    except Exception:
        v = value
    matches = groups_df.index[groups_df[group_key] == v].tolist()
    if matches:
        return int(matches[0]) + 1  # 1-based
    # fallback string compare
    matches = groups_df.index[groups_df[group_key].astype(str) == str(value)].tolist()
    if matches:
        return int(matches[0]) + 1
    return None


def _find_obj_row(df: pd.DataFrame, obj_id: str) -> Optional[pd.Series]:
    if not obj_id:
        return None
    if "obj_id" not in df.columns:
        return None
    m = df["obj_id"].astype(str) == str(obj_id)
    if not m.any():
        return None
    return df[m].iloc[0]


def _cosine(a: np.ndarray, b: np.ndarray) -> float:
    a = a.astype(np.float32, copy=False).reshape(-1)
    b = b.astype(np.float32, copy=False).reshape(-1)
    na = np.linalg.norm(a)
    nb = np.linalg.norm(b)
    if na == 0 or nb == 0:
        return float("nan")
    return float(np.dot(a, b) / (na * nb))


def _render_1v1(df: pd.DataFrame, cfg: dict) -> None:
    obj_a = cfg["obj_a"]
    obj_b = cfg["obj_b"]
    if not obj_a or not obj_b:
        return
    row_a = _find_obj_row(df, obj_a)
    row_b = _find_obj_row(df, obj_b)
    if row_a is None or row_b is None:
        st.warning("1v1ï¼šæœªæ‰¾åˆ° obj_id A æˆ– Bã€‚")
        return

    cols = st.columns(2)
    with cols[0]:
        st.markdown(f"**A: {obj_a}**")
        if "img_url" in row_a:
            st.image(row_a["img_url"], use_container_width=True)
    with cols[1]:
        st.markdown(f"**B: {obj_b}**")
        if "img_url" in row_b:
            st.image(row_b["img_url"], use_container_width=True)

    feat_col = cfg["feature_col"]
    if feat_col not in df.columns:
        st.warning(f"1v1ï¼šç¼ºå°‘ feature åˆ— `{feat_col}`ï¼Œæ— æ³•è®¡ç®—ç›¸ä¼¼åº¦ã€‚")
        return
    fa = row_a.get(feat_col)
    fb = row_b.get(feat_col)
    va = data_utils._parse_feature_vector(fa)  # type: ignore[attr-defined]
    vb = data_utils._parse_feature_vector(fb)  # type: ignore[attr-defined]
    if va is None or vb is None:
        st.warning("1v1ï¼šfeature ç¼ºå¤±æˆ–æ ¼å¼ä¸æ­£ç¡®ã€‚")
        return
    st.metric("Cosine Similarity", f"{_cosine(va, vb):.3f}")


def _render_obj_topk(df: pd.DataFrame, cfg: dict) -> None:
    if cfg["search_mode"] != "obj_id":
        return
    obj_id = cfg["search_value"]
    if not obj_id:
        return
    row = _find_obj_row(df, obj_id)
    if row is None:
        st.warning("obj_id æœç´¢ï¼šæœªæ‰¾åˆ°è¯¥æ ·æœ¬ã€‚")
        return

    st.subheader("obj_id å®šä½")
    st.write(row.to_frame("value"))

    if not cfg["show_topk"]:
        return
    if not faiss_utils.is_faiss_available():
        st.warning("TopK ç›¸ä¼¼ï¼šå½“å‰ç¯å¢ƒæœªå®‰è£… Faissã€‚")
        return

    feat_col = cfg["feature_col"]
    if feat_col not in df.columns:
        st.warning(f"TopK ç›¸ä¼¼ï¼šç¼ºå°‘ feature åˆ— `{feat_col}`ã€‚")
        return

    ph = st.empty()
    prog = ph.progress(0.0, text="å‡†å¤‡ TopK ç›¸ä¼¼æœç´¢ ...")

    try:
        index, row_idx = _ensure_faiss_index(df, cfg, progress=prog)
    except Exception as e:  # noqa: BLE001
        ph.empty()
        st.error(f"TopK ç›¸ä¼¼ï¼šæ„å»º/è·å– Faiss ç´¢å¼•å¤±è´¥ï¼š{e}")
        return

    # Query vector
    q = data_utils._parse_feature_vector(row.get(feat_col))  # type: ignore[attr-defined]
    if q is None:
        ph.empty()
        st.warning("TopK ç›¸ä¼¼ï¼šè¯¥æ ·æœ¬ feature ç¼ºå¤±æˆ–æ ¼å¼ä¸æ­£ç¡®ã€‚")
        return
    q = q.astype(np.float32, copy=False).reshape(1, -1)
    q = faiss_utils.l2_normalize(q)

    prog.progress(0.98, text="Faiss search ...")
    sims, nbrs = index.search(q, int(cfg["sim_topk"]) + 1)
    sims = sims[0]
    nbrs = nbrs[0]

    nbr_df_idx = row_idx[nbrs]
    # Remove self if present
    self_mask = np.array([str(x) == str(row.name) for x in nbr_df_idx], dtype=bool)
    nbr_df_idx = nbr_df_idx[~self_mask]
    sims = sims[~self_mask]

    nbr_df_idx = nbr_df_idx[: int(cfg["sim_topk"])]
    sims = sims[: int(cfg["sim_topk"])]

    ph.empty()
    sub = df.loc[nbr_df_idx].copy()
    # Format similarity for display
    sub["sim"] = [f"{s:.3f}" for s in sims]
    
    st.subheader("TopK ç›¸ä¼¼æ ·æœ¬ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰")
    ui_utils.render_image_grid(
        sub,
        metadata_cols=["sim"] + list(cfg["meta_cols"]),
        ncols=cfg["img_cols"],
        max_images=min(200, int(cfg["sim_topk"])),
    )


def _render_groups_size(df: pd.DataFrame, cfg: dict) -> pd.DataFrame:
    group_key = cfg["group_key"]
    gdf = analysis_utils.group_sizes(df, group_key)
    gdf = gdf.sort_values("size", ascending=not cfg["desc"]).reset_index(drop=True)
    return gdf


def _render_groups_variance(df: pd.DataFrame, cfg: dict) -> pd.DataFrame:
    if not faiss_utils.is_faiss_available():
        st.warning("Varianceï¼šæœªå®‰è£… Faiss ä¹Ÿå¯è®¡ç®—ï¼Œä½†éœ€è¦ feature å¯ç”¨ã€‚")
    feat_col = cfg["feature_col"]
    cache = _analysis_cache()
    ck = _cache_key("variance_groups", get_df_rev(), cfg["group_key"], feat_col, cfg["ok_only"])
    if ck in cache:
        return cache[ck]

    ph = st.empty()
    prog = ph.progress(0.0, text="æå–ç‰¹å¾ ...")

    def feat_cb(frac: float, text: str) -> None:
        prog.progress(min(0.20, 0.20 * float(frac)), text=text)

    feats, row_idx = _get_features(df, feature_col=feat_col, ok_only=cfg["ok_only"], progress_callback=feat_cb)
    prog.progress(0.22, text="è®¡ç®—ç°‡å†…æ–¹å·® ...")

    def var_cb(frac: float, text: str) -> None:
        prog.progress(min(0.98, 0.22 + 0.76 * float(frac)), text=text)

    gdf = analysis_utils.group_variances(
        df,
        group_key=cfg["group_key"],
        feats=feats,
        feats_row_indices=row_idx,
        progress_callback=var_cb,
    )
    gdf = gdf.sort_values("variance", ascending=not cfg["desc"]).reset_index(drop=True)
    prog.progress(1.0, text="æ–¹å·®è®¡ç®—å®Œæˆï¼ˆå·²ç¼“å­˜ï¼‰")
    ph.empty()
    cache[ck] = gdf
    return gdf


def _render_groups_scatter(df: pd.DataFrame, cfg: dict):
    if not faiss_utils.is_faiss_available():
        st.error("Scatter éœ€è¦ Faissï¼šè¯·å®‰è£… faiss-cpu æˆ– faiss-gpuã€‚")
        return []
    feat_col = cfg["feature_col"]
    cache = _analysis_cache()
    ck = _cache_key(
        "scatter_groups",
        get_df_rev(),
        cfg["group_key"],
        feat_col,
        cfg["ok_only"],
        cfg["sim_th"],
        cfg["scatter_topk"],
        cfg.get("dedup_scatter", False),  # include dedup flag in cache key
    )
    if ck in cache:
        return cache[ck]

    ph = st.empty()
    prog = ph.progress(0.0, text="æå–ç‰¹å¾ ...")

    def feat_cb(frac: float, text: str) -> None:
        prog.progress(min(0.20, 0.20 * float(frac)), text=text)

    feats, row_idx = _get_features(df, feature_col=feat_col, ok_only=cfg["ok_only"], progress_callback=feat_cb)
    prog.progress(0.22, text="è®¡ç®—æ•£åº¦ï¼ˆå« Faiss æ£€ç´¢ï¼‰ ...")

    def scatter_cb(frac: float, text: str) -> None:
        prog.progress(min(0.98, 0.22 + 0.76 * float(frac)), text=text)

    groups = analysis_utils.compute_scatter_groups(
        df,
        group_key=cfg["group_key"],
        feats=feats,
        feats_row_indices=row_idx,
        sim_th=float(cfg["sim_th"]),
        sim_topk=int(cfg["scatter_topk"]),
        use_gpu_prefer=cfg["use_gpu_prefer"],
        progress_callback=scatter_cb,
    )

    if cfg.get("dedup_scatter"):
        prog.progress(0.99, text="ç»“æœå»é‡ ...")
        groups = analysis_utils.deduplicate_scatter_groups(groups)

    groups.sort(key=lambda g: g.group_min_sim, reverse=cfg["desc"])
    prog.progress(1.0, text="æ•£åº¦è®¡ç®—å®Œæˆï¼ˆå·²ç¼“å­˜ï¼‰")
    ph.empty()
    cache[ck] = groups
    return groups


def _slice_range(items_len: int, start_1: int, end_1: int) -> tuple[int, int]:
    start = max(0, int(start_1) - 1)
    end = max(start, int(end_1) - 1)
    end = min(end, items_len - 1)
    return start, end


def main() -> None:
    ensure_state()
    st.title("åˆ†æï¼šç°‡è´¨é‡è¯„ä¼°")
    st.caption("æŒ‰å¤§å° / æ–¹å·®(çº¯åº¦) / æ•£åº¦(è·¨ç°‡ç›¸ä¼¼) æ’åºæŸ¥çœ‹ç°‡ï¼Œå¹¶æ”¯æŒæœç´¢ä¸ç›¸ä¼¼åº¦åˆ†æã€‚")

    df = _require_df()
    if df is None:
        return

    cfg = _sidebar_controls(df)

    # 1v1
    has_1v1 = bool(cfg["obj_a"] and cfg["obj_b"])
    if has_1v1:
        st.subheader("1v1 ç›¸ä¼¼åº¦æ¯”è¾ƒ")
        _render_1v1(df, cfg)
        st.divider()
        # Exclusive view: return early
        return

    # obj topk (if requested)
    has_search_obj = (cfg["search_mode"] == "obj_id" and cfg["search_value"])
    if has_search_obj:
        st.subheader(f"æœç´¢ç»“æœ: {cfg['search_value']}")
        _render_obj_topk(df, cfg)
        st.divider()
        # Exclusive view: return early
        return

    st.divider()

    group_key = cfg["group_key"]
    view_mode = cfg["view_mode"]

    # Build groups list
    if view_mode == "Size":
        groups_df = _render_groups_size(df, cfg)
        metric_col = "size"
    elif view_mode == "Variance":
        groups_df = _render_groups_variance(df, cfg)
        metric_col = "variance"
    else:
        scatter_groups = _render_groups_scatter(df, cfg)
        # cluster/label search in scatter mode will search main_label
        if cfg["search_mode"] == "cluster_id / label" and cfg["search_value"]:
            # locate 1-based
            try:
                v = int(cfg["search_value"])
            except Exception:
                v = cfg["search_value"]
            found = None
            for i, g in enumerate(scatter_groups):
                if g.main_label == v or str(g.main_label) == str(v):
                    found = i + 1
                    break
            if found is not None:
                st.sidebar.success(f"å®šä½åˆ°ä¸»ç°‡åºå·: {found}")
                cfg["range_start"] = found
                cfg["range_end"] = found
        start, end = _slice_range(len(scatter_groups), cfg["range_start"], cfg["range_end"])
        st.subheader(f"{view_mode}æ’åº | ç°‡æ ‡ç­¾:`{group_key}` | æ€»æ•°: {len(scatter_groups):,} | å½“å‰åºå· {start+1}ï½{end+1}")

        idx_map = _get_group_index_map(df, group_key)
        limit = int(cfg["per_group_images"])
        rand = bool(cfg["random_sample"])
        cand_limit = int(cfg.get("cand_limit", 3))

        for i in range(start, end + 1):
            g = scatter_groups[i]
            # Outer container for the whole scatter group
            with st.container(border=True):
                # Group Header
                _render_cluster_header(
                    title=f"#{i+1} Scatter Group",
                    subtitle=f"Main: {g.main_label} | Candidates: {len(g.candidates)}",
                    chips=[f"min_sim={g.group_min_sim:.3f}"]
                )
                
                # 1. Main Cluster
                main_indices = idx_map.get(g.main_label, [])
                main_size = len(main_indices)
                
                st.markdown(f"##### ğŸŸ¢ Main Cluster: {g.main_label} <span style='color:grey;font-size:0.9em'>(size={main_size:,})</span>", unsafe_allow_html=True)
                
                # Expand controls for Main Cluster
                expanded, page, per_page = _get_expand_state(
                    total=main_size,
                    collapsed_n=limit,
                    cfg=cfg,
                    key_parts=["scatter_main", i, g.main_label]
                )
                
                if not expanded:
                    show = _sample_by_indices(df, main_indices, n=limit, random_sample=rand, seed=_stable_seed(f"main::{g.main_label}"))
                    ui_utils.render_image_grid(show, metadata_cols=list(cfg["meta_cols"]), ncols=cfg["img_cols"], max_images=len(show))
                else:
                    p_start = (page - 1) * per_page
                    p_end = min(p_start + per_page, main_size)
                    pg_indices = _slice_indices(main_indices, p_start, p_end)
                    ui_utils.render_image_grid(df.loc[pg_indices], metadata_cols=list(cfg["meta_cols"]), ncols=cfg["img_cols"], max_images=len(pg_indices))
                
                _render_expand_trigger(total=main_size, collapsed_n=limit, cfg=cfg, key_parts=["scatter_main", i, g.main_label])

                # 2. Candidate Clusters
                candidates_to_show = g.candidates[:cand_limit]
                for c_idx, c in enumerate(candidates_to_show):
                    # Use padding instead of divider for visual separation
                    st.markdown('<div style="margin-top: 1.2rem;"></div>', unsafe_allow_html=True)
                    c_indices = idx_map.get(c.label, [])
                    c_size = len(c_indices)
                    st.markdown(
                        f"##### ğŸŸ  Candidate #{c_idx+1}: {c.label} "
                        f"<span style='color:grey;font-size:0.9em'>(size={c_size:,}, min_sim={c.min_sim:.3f})</span>", 
                        unsafe_allow_html=True
                    )
                    
                    # Expand controls for THIS candidate
                    expanded_c, page_c, per_page_c = _get_expand_state(
                        total=c_size,
                        collapsed_n=limit,
                        cfg=cfg,
                        key_parts=["scatter_cand", i, g.main_label, c.label]
                    )
                    
                    if not expanded_c:
                        c_show = _sample_by_indices(df, c_indices, n=limit, random_sample=rand, seed=_stable_seed(f"cand::{g.main_label}::{c.label}"))
                        ui_utils.render_image_grid(c_show, metadata_cols=list(cfg["meta_cols"]), ncols=cfg["img_cols"], max_images=len(c_show))
                    else:
                        p_start_c = (page_c - 1) * per_page_c
                        p_end_c = min(p_start_c + per_page_c, c_size)
                        pg_indices_c = _slice_indices(c_indices, p_start_c, p_end_c)
                        ui_utils.render_image_grid(df.loc[pg_indices_c], metadata_cols=list(cfg["meta_cols"]), ncols=cfg["img_cols"], max_images=len(pg_indices_c))
                        
                    _render_expand_trigger(total=c_size, collapsed_n=limit, cfg=cfg, key_parts=["scatter_cand", i, g.main_label, c.label])

                if len(g.candidates) > cand_limit:
                    st.caption(f"... è¿˜æœ‰ {len(g.candidates) - cand_limit} ä¸ªå€™é€‰ç°‡è¢«éšè— (å¯åœ¨ä¾§è¾¹æ è°ƒæ•´æ˜¾ç¤ºæ•°é‡)")
            
            st.divider()
        return

    # Cluster/label search (non-scatter)
    if cfg["search_mode"] == "cluster_id / label" and cfg["search_value"]:
        pos = _locate_by_label(groups_df, group_key, cfg["search_value"])
        if pos is None:
            st.sidebar.warning("æœªæ‰¾åˆ°å¯¹åº”ç°‡ã€‚")
        else:
            st.sidebar.success(f"å®šä½åˆ°ç°‡åºå·: {pos}")
            cfg["range_start"] = pos
            cfg["range_end"] = pos

    start, end = _slice_range(len(groups_df), cfg["range_start"], cfg["range_end"])
    st.subheader(f"{view_mode}æ’åº | ç°‡æ ‡ç­¾:`{group_key}` | æ€»æ•°: {len(groups_df):,} | å½“å‰åºå· {start+1}ï½{end+1}")

    idx_map = _get_group_index_map(df, group_key)
    limit = int(cfg["per_group_images"])
    rand = bool(cfg["random_sample"])

    for i in range(start, end + 1):
        row = groups_df.iloc[i]
        gid = row[group_key]
        metric_val = row.get(metric_col)
        indices = idx_map.get(gid, [])
        size = int(len(indices))
        
        chips = [f"size={size:,}"]
        if view_mode == "Variance":
            chips.append(f"variance={float(metric_val):.6f}")

        with st.container(border=True):
            _render_cluster_header(
                title=f"#{i+1} {group_key}={gid}",
                subtitle=f"View: {view_mode}",
                chips=chips
            )
            
            limit = int(cfg["per_group_images"])
            rand = bool(cfg["random_sample"])
            
            expanded, page, per_page = _get_expand_state(
                total=size,
                collapsed_n=limit,
                cfg=cfg,
                key_parts=["group", i, gid]
            )
            
            if not expanded:
                show = _sample_by_indices(df, indices, n=limit, random_sample=rand, seed=_stable_seed(gid))
                st.caption(f"é¢„è§ˆæ¨¡å¼ (ä¸Šé™ {limit}) | å½“å‰å±•ç¤º {len(show):,}/{size:,}")
                ui_utils.render_image_grid(
                    show,
                    metadata_cols=list(cfg["meta_cols"]),
                    ncols=cfg["img_cols"],
                    max_images=len(show)
                )
            else:
                p_start = (page - 1) * per_page
                p_end = min(p_start + per_page, size)
                pg_indices = _slice_indices(indices, p_start, p_end)
                st.caption(f"å…¨é‡æµè§ˆ | ç¬¬ {page} é¡µ | å±•ç¤º {p_start+1}..{p_end} (æ€»è®¡ {size:,})")
                ui_utils.render_image_grid(
                    df.loc[pg_indices],
                    metadata_cols=list(cfg["meta_cols"]),
                    ncols=cfg["img_cols"],
                    max_images=len(pg_indices)
                )
            
            _render_expand_trigger(
                total=size,
                collapsed_n=limit,
                cfg=cfg,
                key_parts=["group", i, gid]
            )
        st.divider()
        # st.divider() is implicit via container borders, but we can add space
        st.write("")

if __name__ == "__main__":
    main()

